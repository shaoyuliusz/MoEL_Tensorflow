{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0188a072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wangshihang/Python/Experiment/MoEL_tensorflow\n"
     ]
    }
   ],
   "source": [
    "cd /Users/wangshihang/Python/Experiment/MoEL_tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fdab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, init_index2word):\n",
    "        self.word2index = {str(v): int(k) for k, v in init_index2word.items()}\n",
    "        self.word2count = {str(v): 1 for k, v in init_index2word.items()}\n",
    "        self.index2word = init_index2word \n",
    "        self.n_words = len(init_index2word)  # Count default tokens\n",
    "      \n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence:\n",
    "            self.index_word(word.strip())\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "\n",
    "def clean(sentence, word_pairs):\n",
    "    sentence = sentence.lower()\n",
    "    for k, v in word_pairs.items():\n",
    "        sentence = sentence.replace(k,v)\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def read_langs(vocab):\n",
    "    word_pairs = {\"it's\":\"it is\", \"don't\":\"do not\", \"doesn't\":\"does not\", \"didn't\":\"did not\", \"you'd\":\"you would\", \"you're\":\"you are\", \"you'll\":\"you will\", \"i'm\":\"i am\", \"they're\":\"they are\", \"that's\":\"that is\", \"what's\":\"what is\", \"couldn't\":\"could not\", \"i've\":\"i have\", \"we've\":\"we have\", \"can't\":\"cannot\", \"i'd\":\"i would\", \"i'd\":\"i would\", \"aren't\":\"are not\", \"isn't\":\"is not\", \"wasn't\":\"was not\", \"weren't\":\"were not\", \"won't\":\"will not\", \"there's\":\"there is\", \"there're\":\"there are\"}\n",
    "    train_context = np.load('empathetic-dialogue/sys_dialog_texts.train.npy',allow_pickle=True)\n",
    "    train_target = np.load('empathetic-dialogue/sys_target_texts.train.npy',allow_pickle=True)\n",
    "    train_emotion = np.load('empathetic-dialogue/sys_emotion_texts.train.npy',allow_pickle=True)\n",
    "    train_situation = np.load('empathetic-dialogue/sys_situation_texts.train.npy',allow_pickle=True)\n",
    "\n",
    "    dev_context = np.load('empathetic-dialogue/sys_dialog_texts.dev.npy',allow_pickle=True)\n",
    "    dev_target = np.load('empathetic-dialogue/sys_target_texts.dev.npy',allow_pickle=True)\n",
    "    dev_emotion = np.load('empathetic-dialogue/sys_emotion_texts.dev.npy',allow_pickle=True)\n",
    "    dev_situation = np.load('empathetic-dialogue/sys_situation_texts.dev.npy',allow_pickle=True)\n",
    "    \n",
    "    test_context = np.load('empathetic-dialogue/sys_dialog_texts.test.npy',allow_pickle=True)\n",
    "    test_target = np.load('empathetic-dialogue/sys_target_texts.test.npy',allow_pickle=True)\n",
    "    test_emotion = np.load('empathetic-dialogue/sys_emotion_texts.test.npy',allow_pickle=True)\n",
    "    test_situation = np.load('empathetic-dialogue/sys_situation_texts.test.npy',allow_pickle=True)\n",
    "\n",
    "    data_train = {'context':[],'target':[],'emotion':[], 'situation':[]}\n",
    "    data_dev = {'context':[],'target':[],'emotion':[], 'situation':[]}\n",
    "    data_test = {'context':[],'target':[],'emotion':[], 'situation':[]}\n",
    "\n",
    "    for context in train_context:\n",
    "        u_list = []\n",
    "        for u in context:\n",
    "            u = clean(u, word_pairs)\n",
    "            u_list.append(u)\n",
    "            vocab.index_words(u)\n",
    "        data_train['context'].append(u_list)\n",
    "    for target in train_target:\n",
    "        target = clean(target, word_pairs)\n",
    "        data_train['target'].append(target)\n",
    "        vocab.index_words(target)\n",
    "    for situation in train_situation:\n",
    "        situation = clean(situation, word_pairs)\n",
    "        data_train['situation'].append(situation)\n",
    "        vocab.index_words(situation)\n",
    "    for emotion in train_emotion:\n",
    "        data_train['emotion'].append(emotion)\n",
    "    assert len(data_train['context']) == len(data_train['target']) == len(data_train['emotion']) == len(data_train['situation'])\n",
    "\n",
    "    for context in dev_context:\n",
    "        u_list = []\n",
    "        for u in context:\n",
    "            u = clean(u, word_pairs)\n",
    "            u_list.append(u)\n",
    "            vocab.index_words(u)\n",
    "        data_dev['context'].append(u_list)\n",
    "    for target in dev_target:\n",
    "        target = clean(target, word_pairs)\n",
    "        data_dev['target'].append(target)\n",
    "        vocab.index_words(target)\n",
    "    for situation in dev_situation:\n",
    "        situation = clean(situation, word_pairs)\n",
    "        data_dev['situation'].append(situation)\n",
    "        vocab.index_words(situation)\n",
    "    for emotion in dev_emotion:\n",
    "        data_dev['emotion'].append(emotion)\n",
    "    assert len(data_dev['context']) == len(data_dev['target']) == len(data_dev['emotion']) == len(data_dev['situation'])\n",
    "\n",
    "    for context in test_context:\n",
    "        u_list = []\n",
    "        for u in context:\n",
    "            u = clean(u, word_pairs)\n",
    "            u_list.append(u)\n",
    "            vocab.index_words(u)\n",
    "        data_test['context'].append(u_list)\n",
    "    for target in test_target:\n",
    "        target = clean(target, word_pairs)\n",
    "        data_test['target'].append(target)\n",
    "        vocab.index_words(target)\n",
    "    for situation in test_situation:\n",
    "        situation = clean(situation, word_pairs)\n",
    "        data_test['situation'].append(situation)\n",
    "        vocab.index_words(situation)\n",
    "    for emotion in test_emotion:\n",
    "        data_test['emotion'].append(emotion)\n",
    "    assert len(data_test['context']) == len(data_test['target']) == len(data_test['emotion']) == len(data_test['situation'])\n",
    "    return data_train, data_dev, data_test, vocab\n",
    "\n",
    "#load_dataset returns 3 dictionaries data_tra, data_val, data_tst and vocab <__main__.Lang at 0x7fdeaf8be0d0>\n",
    "\n",
    "def load_dataset():\n",
    "    if(os.path.exists('empathetic-dialogue/dataset_preproc.p')):\n",
    "        print(\"LOADING empathetic_dialogue\")\n",
    "        with open('empathetic-dialogue/dataset_preproc.p', \"rb\") as f:\n",
    "            [data_tra, data_val, data_tst, vocab] = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Building dataset...\")\n",
    "        data_tra, data_val, data_tst, vocab  = read_langs(vocab=Lang({0: \"UNK\", 1: \"PAD\", 2: \"EOS\", 3: \"SOS\",\n",
    "                                                                      4:\"USR\", 5:\"SYS\", 6:\"CLS\"})) \n",
    "        with open('empathetic-dialogue/dataset_preproc.p', \"wb\") as f:\n",
    "            pickle.dump([data_tra, data_val, data_tst, vocab], f)\n",
    "            print(\"Saved PICKLE\")\n",
    "    for i in range(3):\n",
    "        print('[situation]:', ' '.join(data_tra['situation'][i]))\n",
    "        print('[emotion]:', data_tra['emotion'][i])\n",
    "        print('[context]:', [' '.join(u) for u in data_tra['context'][i]])\n",
    "        print('[target]:', ' '.join(data_tra['target'][i]))\n",
    "        print(\" \")\n",
    "    return data_tra, data_val, data_tst, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1996d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset...\n",
      "Saved PICKLE\n",
      "[situation]: i remember going to the fireworks with my best friend . there was a lot of people , but it only felt like us in the world .\n",
      "[emotion]: sentimental\n",
      "[context]: ['i remember going to see the fireworks with my best friend . it was the first time we ever spent time alone together . although there was a lot of people , we felt like the only people in the world .']\n",
      "[target]: was this a friend you were in love with , or just a best friend ?\n",
      " \n",
      "[situation]: i remember going to the fireworks with my best friend . there was a lot of people , but it only felt like us in the world .\n",
      "[emotion]: sentimental\n",
      "[context]: ['i remember going to see the fireworks with my best friend . it was the first time we ever spent time alone together . although there was a lot of people , we felt like the only people in the world .', 'was this a friend you were in love with , or just a best friend ?', 'this was a best friend . i miss her .']\n",
      "[target]: where has she gone ?\n",
      " \n",
      "[situation]: i remember going to the fireworks with my best friend . there was a lot of people , but it only felt like us in the world .\n",
      "[emotion]: sentimental\n",
      "[context]: ['i remember going to see the fireworks with my best friend . it was the first time we ever spent time alone together . although there was a lot of people , we felt like the only people in the world .', 'was this a friend you were in love with , or just a best friend ?', 'this was a best friend . i miss her .', 'where has she gone ?', 'we no longer talk .']\n",
      "[target]: oh was this something that happened because of an argument ?\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "pairs_tra, pairs_val, pairs_tst, vocab = load_dataset()\n",
    "\n",
    "# data: 4-key dictionary\n",
    "def process_item(data):\n",
    "    \"\"\"Returns one data pair (source and target).\"\"\"\n",
    "    emo_map = {\n",
    "        'surprised': 0, 'excited': 1, 'annoyed': 2, 'proud': 3, 'angry': 4, 'sad': 5, 'grateful': 6, 'lonely': 7,\n",
    "        'impressed': 8, 'afraid': 9, 'disgusted': 10, 'confident': 11, 'terrified': 12, 'hopeful': 13, 'anxious': 14, 'disappointed': 15,\n",
    "        'joyful': 16, 'prepared': 17, 'guilty': 18, 'furious': 19, 'nostalgic': 20, 'jealous': 21, 'anticipating': 22, 'embarrassed': 23,\n",
    "        'content': 24, 'devastated': 25, 'sentimental': 26, 'caring': 27, 'trusting': 28, 'ashamed': 29, 'apprehensive': 30, 'faithful': 31}\n",
    "    ls_item = []\n",
    "    for index in range(0, len(data['context'])):\n",
    "        \n",
    "        item = {}\n",
    "        item[\"context_text\"] = data[\"context\"][index]\n",
    "        item[\"target_text\"] = data[\"target\"][index]\n",
    "        item[\"emotion_text\"] = data[\"emotion\"][index]\n",
    "\n",
    "        item[\"context\"], item[\"context_mask\"] = preprocess(item[\"context_text\"])\n",
    "\n",
    "        item[\"target\"] = preprocess(item[\"target_text\"], anw=True)\n",
    "        item[\"emotion\"], item[\"emotion_label\"] = preprocess_emo(item[\"emotion_text\"], emo_map)\n",
    "        ls_item.append(item)\n",
    "    \n",
    "    ls_item.sort(key=lambda x: len(x[\"context\"]), reverse=True)\n",
    "    \n",
    "\n",
    "    item_info = {} #item_info is a 8 key dictionary of lists\n",
    "    for key in ls_item[0].keys():\n",
    "        item_info[key] = [d[key] for d in ls_item]\n",
    "    #print('CONTENT OF item_info context:', item_info['context'])\n",
    "    return item_info #dict of lists\n",
    "\n",
    "def preprocess(arr, anw=False):\n",
    "    \"\"\"Converts words to ids.\"\"\"\n",
    "    if(anw):\n",
    "        sequence = [vocab.word2index[word] if word in vocab.word2index else 0 for word in arr] + [2]\n",
    "        return sequence\n",
    "    else:\n",
    "        X_dial = [6]\n",
    "        X_mask = [6]\n",
    "    for i, sentence in enumerate(arr):\n",
    "            X_dial += [vocab.word2index[word] if word in vocab.word2index else 0 for word in sentence] #修改\n",
    "            spk = vocab.word2index[\"USR\"] if i % 2 == 0 else vocab.word2index[\"SYS\"]\n",
    "            X_mask += [spk for _ in range(len(sentence))] #修改\n",
    "    assert len(X_dial) == len(X_mask)\n",
    "\n",
    "    return X_dial, X_mask\n",
    "\n",
    "def preprocess_emo(emotion, emo_map):\n",
    "    program = [0]*len(emo_map)\n",
    "    program[emo_map[emotion]] = 1\n",
    "\n",
    "    return program, emo_map[emotion]\n",
    "\n",
    "   \n",
    "#each item has 7 tensors\n",
    "def convert_to_dataset(item_info,batch_size = 16):\n",
    "    \"\"\"convert to Tensorflow data.DataSet\"\"\"\n",
    "    def _element_length_fn(a,b,c,d,e,f,g):\n",
    "         return array_ops.shape(a)[0]\n",
    "        \n",
    "    def generator():\n",
    "    #dt = [data_tst.__getitem__(i) for i in range(0, 30)]\n",
    "        for i in range(0, len(item_info['context'])):\n",
    "            # creates x’s and y’s for the dataset.\n",
    "            input_batch = item_info['context'][i] #var len [...]\n",
    "            input_length = [len(item_info['context'][i])] #修改\n",
    "            mask_input = item_info['context_mask'][i] #var len [...]\n",
    "            target_batch = item_info['target'][i] #var len [...]\n",
    "            target_length = [len(item_info['target'][i])] #[int]\n",
    "            target_program = item_info['emotion'][i] #var len [...]\n",
    "            program_label = [item_info['emotion_label'][i]] #[int]\n",
    "            yield input_batch, input_length, mask_input, target_batch, target_length, target_program, program_label\n",
    "    \n",
    "    toy_dataset = tf.data.Dataset.from_generator(\n",
    "         generator,\n",
    "         output_signature=(\n",
    "             tf.TensorSpec(shape=([None]), dtype=tf.int32),\n",
    "             tf.TensorSpec(shape=(1,), dtype=tf.int32),\n",
    "             tf.TensorSpec(shape=([None]), dtype=tf.int32),\n",
    "             tf.TensorSpec(shape=([None]), dtype=tf.int32),\n",
    "             tf.TensorSpec(shape=(1,), dtype=tf.int32),\n",
    "             tf.TensorSpec(shape=([None]), dtype=tf.int32),\n",
    "             tf.TensorSpec(shape=(1,), dtype=tf.int32)\n",
    "         ))\n",
    "    #print('LEN CONTEXT',len(item_info['context']))\n",
    "    bound_num = len(item_info['context']) // batch_size\n",
    "    \n",
    "    #print('BOUND NUM',bound_num)\n",
    "    #let's use context length for bucket_by_sequence_length\n",
    "    toy_boundaries = [len(x) for i, x in enumerate(item_info['context']) if i% bound_num == 0]\n",
    "    #[batch_size] * len(bucket_boundaries) + 1\n",
    "    toy_bucket_batch_sizes = [batch_size] * (len(toy_boundaries) + 1)\n",
    "\n",
    "    #this is similar to the \"merge\" function to pad lists for each batch\n",
    "    dataset = toy_dataset.apply(tf.data.experimental.bucket_by_sequence_length(_element_length_fn, \n",
    "                                                                           bucket_boundaries = toy_boundaries,\n",
    "                                                                           bucket_batch_sizes = toy_bucket_batch_sizes,\n",
    "                                                                           drop_remainder=True,\n",
    "                                                                           #padding_values=1,\n",
    "                                                                           pad_to_bucket_boundary=False))\n",
    "    return dataset\n",
    "\n",
    "def prepare_data_seq(batch_size):\n",
    "    pairs_tra, pairs_val, pairs_tst, vocab = load_dataset()\n",
    "    \n",
    "    emo_map = {\n",
    "        'surprised': 0, 'excited': 1, 'annoyed': 2, 'proud': 3, 'angry': 4, 'sad': 5, 'grateful': 6, 'lonely': 7,\n",
    "        'impressed': 8, 'afraid': 9, 'disgusted': 10, 'confident': 11, 'terrified': 12, 'hopeful': 13, 'anxious': 14, 'disappointed': 15,\n",
    "        'joyful': 16, 'prepared': 17, 'guilty': 18, 'furious': 19, 'nostalgic': 20, 'jealous': 21, 'anticipating': 22, 'embarrassed': 23,\n",
    "        'content': 24, 'devastated': 25, 'sentimental': 26, 'caring': 27, 'trusting': 28, 'ashamed': 29, 'apprehensive': 30, 'faithful': 31}\n",
    "    \n",
    "    item_info_tra = process_item(pairs_tra)\n",
    "    item_info_val = process_item(pairs_val)\n",
    "    item_info_tst = process_item(pairs_tst)\n",
    "    \n",
    "    data_loader_tra = convert_to_dataset(item_info_tra)\n",
    "    data_loader_val = convert_to_dataset(item_info_val)\n",
    "    data_loader_tst = convert_to_dataset(item_info_tst)\n",
    "    \n",
    "    \n",
    "    return data_loader_tra, data_loader_val, data_loader_tst, vocab, len(emo_map)\n",
    "\n",
    "\n",
    "def test():\n",
    "    pairs_tra, pairs_val, pairs_tst, vocab = load_dataset()\n",
    "    \n",
    "    item_info_tra = process_item(pairs_tra)\n",
    "    item_info_val = process_item(pairs_val)\n",
    "    item_info_tst = process_item(pairs_tst)\n",
    "    \n",
    "    data_loader_tra = convert_to_dataset(item_info_tra)\n",
    "    data_loader_val = convert_to_dataset(item_info_val)\n",
    "    data_loader_tst = convert_to_dataset(item_info_tst)\n",
    "    #vocab, len(dataset_train.emo_map)\n",
    "    \n",
    "    return data_loader_tra, data_loader_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06b2badf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING empathetic_dialogue\n",
      "[situation]: i remember going to the fireworks with my best friend . there was a lot of people , but it only felt like us in the world .\n",
      "[emotion]: sentimental\n",
      "[context]: ['i remember going to see the fireworks with my best friend . it was the first time we ever spent time alone together . although there was a lot of people , we felt like the only people in the world .']\n",
      "[target]: was this a friend you were in love with , or just a best friend ?\n",
      " \n",
      "[situation]: i remember going to the fireworks with my best friend . there was a lot of people , but it only felt like us in the world .\n",
      "[emotion]: sentimental\n",
      "[context]: ['i remember going to see the fireworks with my best friend . it was the first time we ever spent time alone together . although there was a lot of people , we felt like the only people in the world .', 'was this a friend you were in love with , or just a best friend ?', 'this was a best friend . i miss her .']\n",
      "[target]: where has she gone ?\n",
      " \n",
      "[situation]: i remember going to the fireworks with my best friend . there was a lot of people , but it only felt like us in the world .\n",
      "[emotion]: sentimental\n",
      "[context]: ['i remember going to see the fireworks with my best friend . it was the first time we ever spent time alone together . although there was a lot of people , we felt like the only people in the world .', 'was this a friend you were in love with , or just a best friend ?', 'this was a best friend . i miss her .', 'where has she gone ?', 'we no longer talk .']\n",
      "[target]: oh was this something that happened because of an argument ?\n",
      " \n"
     ]
    }
   ],
   "source": [
    "data_loader_tra, data_loader_val, data_loader_tst, vocab, program_number = prepare_data_seq(batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea233631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_infinite(dataloader):\n",
    "    while True:\n",
    "        for x in dataloader:\n",
    "            yield x\n",
    "data_iter = make_infinite(data_loader_tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd975106",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 334)\n",
      "(16, 1)\n",
      "(16, 334)\n",
      "(16, 72)\n",
      "(16, 1)\n",
      "(16, 32)\n",
      "(16, 1)\n",
      "...........\n",
      "(16, 209)\n",
      "(16, 1)\n",
      "(16, 209)\n",
      "(16, 100)\n",
      "(16, 1)\n",
      "(16, 32)\n",
      "(16, 1)\n",
      "...........\n",
      "(16, 192)\n",
      "(16, 1)\n",
      "(16, 192)\n",
      "(16, 82)\n",
      "(16, 1)\n",
      "(16, 32)\n",
      "(16, 1)\n",
      "...........\n",
      "(16, 182)\n",
      "(16, 1)\n",
      "(16, 182)\n",
      "(16, 36)\n",
      "(16, 1)\n",
      "(16, 32)\n",
      "(16, 1)\n",
      "...........\n",
      "(16, 171)\n",
      "(16, 1)\n",
      "(16, 171)\n",
      "(16, 114)\n",
      "(16, 1)\n",
      "(16, 32)\n",
      "(16, 1)\n",
      "...........\n",
      "(16, 164)\n",
      "(16, 1)\n",
      "(16, 164)\n",
      "(16, 85)\n",
      "(16, 1)\n",
      "(16, 32)\n",
      "(16, 1)\n",
      "...........\n",
      "(16, 161)\n",
      "(16, 1)\n",
      "(16, 161)\n",
      "(16, 38)\n",
      "(16, 1)\n",
      "(16, 32)\n",
      "(16, 1)\n",
      "...........\n",
      "(16, 156)\n",
      "(16, 1)\n",
      "(16, 156)\n",
      "(16, 103)\n",
      "(16, 1)\n",
      "(16, 32)\n",
      "(16, 1)\n",
      "...........\n",
      "(16, 151)\n",
      "(16, 1)\n",
      "(16, 151)\n",
      "(16, 51)\n",
      "(16, 1)\n",
      "(16, 32)\n",
      "(16, 1)\n",
      "...........\n",
      "(16, 148)\n",
      "(16, 1)\n",
      "(16, 148)\n",
      "(16, 78)\n",
      "(16, 1)\n",
      "(16, 32)\n",
      "(16, 1)\n",
      "...........\n",
      "(16, 145)\n",
      "(16, 1)\n",
      "(16, 145)\n",
      "(16, 85)\n",
      "(16, 1)\n",
      "(16, 32)\n",
      "(16, 1)\n",
      "...........\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in data_iter:\n",
    "    for k in i:\n",
    "        print(k.shape)\n",
    "    count+=1\n",
    "    print('...........')\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "197ca952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EncoderLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Represents one Encoder layer of the Transformer Encoder\n",
    "    Refer Fig. 1 in https://arxiv.org/pdf/1706.03762.pdf\n",
    "    NOTE: The layer normalization step has been moved to the input as per latest version of T2T\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, total_key_depth, total_value_depth, filter_size, num_heads,\n",
    "                 bias_mask=None, layer_dropout=0.0, attention_dropout=0.0, relu_dropout=0.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            hidden_size: Hidden size\n",
    "            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n",
    "            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n",
    "            output_depth: Size last dimension of the final output\n",
    "            filter_size: Hidden size of the middle layer in FFN\n",
    "            num_heads: Number of attention heads\n",
    "            bias_mask: Masking tensor to prevent connections to future elements\n",
    "            layer_dropout: Dropout for this layer\n",
    "            attention_dropout: Dropout probability after attention (Should be non-zero only during training)\n",
    "            relu_dropout: Dropout probability after relu in FFN (Should be non-zero only during training)\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(hidden_size, total_key_depth, total_value_depth, \n",
    "                                                       hidden_size, num_heads, bias_mask, attention_dropout)\n",
    "        \n",
    "        self.positionwise_feed_forward = PositionwiseFeedForward(hidden_size, filter_size, hidden_size,\n",
    "                                                                 layer_config='cc', padding = 'both', \n",
    "                                                                 dropout=relu_dropout)\n",
    "        self.dropout = layers.Dropout(layer_dropout)\n",
    "        self.layer_norm_mha = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm_ffn = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def __call__(self, inputs, mask=None, training=True):\n",
    "        x = inputs\n",
    "        \n",
    "        # Layer Normalization\n",
    "        x_norm = self.layer_norm_mha(x)\n",
    "        # Multi-head attention\n",
    "        y, _ = self.multi_head_attention(x_norm, x_norm, x_norm, mask)\n",
    "        # Dropout and residual\n",
    "        x = self.dropout(x + y, training=training)\n",
    "        # Layer Normalization\n",
    "        x_norm = self.layer_norm_ffn(x)\n",
    "        # Positionwise Feedforward\n",
    "        y = self.positionwise_feed_forward(x_norm)\n",
    "        # Dropout and residual\n",
    "        y = self.dropout(x + y, training=training)\n",
    "        # y = self.layer_norm_end(y)\n",
    "        return y\n",
    "    \n",
    "class DecoderLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Represents one Decoder layer of the Transformer Decoder\n",
    "    Refer Fig. 1 in https://arxiv.org/pdf/1706.03762.pdf\n",
    "    NOTE: The layer normalization step has been moved to the input as per latest version of T2T\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, total_key_depth, total_value_depth, filter_size, num_heads,\n",
    "                 bias_mask, layer_dropout=0.0, attention_dropout=0.0, relu_dropout=0.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            hidden_size: Hidden size\n",
    "            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n",
    "            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n",
    "            output_depth: Size last dimension of the final output\n",
    "            filter_size: Hidden size of the middle layer in FFN\n",
    "            num_heads: Number of attention heads\n",
    "            bias_mask: Masking tensor to prevent connections to future elements\n",
    "            layer_dropout: Dropout for this layer\n",
    "            attention_dropout: Dropout probability after attention (Should be non-zero only during training)\n",
    "            relu_dropout: Dropout probability after relu in FFN (Should be non-zero only during training)\n",
    "        \"\"\" \n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.multi_head_attention_dec = MultiHeadAttention(hidden_size, total_key_depth, total_value_depth, \n",
    "                                                       hidden_size, num_heads, bias_mask, attention_dropout)\n",
    "\n",
    "        self.multi_head_attention_enc_dec = MultiHeadAttention(hidden_size, total_key_depth, total_value_depth, \n",
    "                                                       hidden_size, num_heads, None, attention_dropout)\n",
    "        \n",
    "        self.positionwise_feed_forward = PositionwiseFeedForward(hidden_size, filter_size, hidden_size,\n",
    "                                                                 layer_config='cc', padding = 'left', \n",
    "                                                                 dropout=relu_dropout)\n",
    "        self.dropout = layers.Dropout(layer_dropout)\n",
    "        self.layer_norm_mha_dec = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm_mha_enc = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm_ffn = layers.LayerNormalization(epsilon=1e-6)\n",
    "        # self.layer_norm_end = layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    def __call__(self, inputs, training=True):\n",
    "        \"\"\"\n",
    "        NOTE: Inputs is a tuple consisting of decoder inputs and encoder output\n",
    "        \"\"\"\n",
    "\n",
    "        x, encoder_outputs, attention_weight, mask = inputs\n",
    "        mask_src, dec_mask = mask\n",
    "        \n",
    "        # Layer Normalization before decoder self attention\n",
    "        x_norm = self.layer_norm_mha_dec(x)\n",
    "        # Masked Multi-head attention\n",
    "        y, _ = self.multi_head_attention_dec(x_norm, x_norm, x_norm, dec_mask)\n",
    "        # Dropout and residual after self-attention\n",
    "        x = self.dropout(x + y, training=training)\n",
    "        # Layer Normalization before encoder-decoder attention\n",
    "        x_norm = self.layer_norm_mha_enc(x)\n",
    "        # Multi-head encoder-decoder attention\n",
    "        y, attention_weight = self.multi_head_attention_enc_dec(x_norm, encoder_outputs, encoder_outputs, mask_src)\n",
    "        # Dropout and residual after encoder-decoder attention\n",
    "        x = self.dropout(x + y, training=training)\n",
    "        # Layer Normalization\n",
    "        x_norm = self.layer_norm_ffn(x)\n",
    "        # Positionwise Feedforward\n",
    "        y = self.positionwise_feed_forward(x_norm)\n",
    "        # Dropout and residual after positionwise feed forward layer\n",
    "        y = self.dropout(x + y, training=training)\n",
    "        # y = self.layer_norm_end(y)\n",
    "        # Return encoder outputs as well to work with tf.keras.models.Sequential\n",
    "        return y, encoder_outputs, attention_weight, mask\n",
    "\n",
    "class MultiExpertMultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, num_experts, input_depth, total_key_depth, total_value_depth, output_depth, \n",
    "                 num_heads, bias_mask=None, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            expert_num: Number of experts\n",
    "            input_depth: Size of last dimension of input\n",
    "            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n",
    "            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n",
    "            output_depth: Size last dimension of the final output\n",
    "            num_heads: Number of attention heads\n",
    "            bias_mask: Masking tensor to prevent connections to future elements\n",
    "            dropout: Dropout probability (Should be non-zero only during training)\n",
    "        \"\"\"\n",
    "        super(MultiExpertMultiHeadAttention, self).__init__()\n",
    "        # Checks borrowed from \n",
    "        # https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py\n",
    "        if total_key_depth % num_heads != 0:\n",
    "            print(\"Key depth (%d) must be divisible by the number of \"\n",
    "                             \"attention heads (%d).\" % (total_key_depth, num_heads))\n",
    "            total_key_depth = total_key_depth - (total_key_depth % num_heads)\n",
    "        if total_value_depth % num_heads != 0:\n",
    "            print(\"Value depth (%d) must be divisible by the number of \"\n",
    "                             \"attention heads (%d).\" % (total_value_depth, num_heads))\n",
    "            total_value_depth = total_value_depth - (total_value_depth % num_heads)\n",
    "        self.num_experts = num_experts\n",
    "        self.num_heads = num_heads\n",
    "        self.query_scale = (total_key_depth//num_heads)**-0.5 ## sqrt\n",
    "        self.bias_mask = bias_mask\n",
    "        \n",
    "        # Key and query depth will be same\n",
    "        self.query_linear = layers.Dense(total_key_depth*num_experts, use_bias=False)\n",
    "        self.key_linear = layers.Dense(total_key_depth*num_experts, use_bias=False)\n",
    "        self.value_linear = layers.Dense(total_value_depth*num_experts, use_bias=False)\n",
    "        self.output_linear = layers.Dense(output_depth*num_experts, use_bias=False)\n",
    "        \n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "    \n",
    "    def _split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Split x such to add an extra num_heads dimension\n",
    "        Input:\n",
    "            x: a Tensor with shape [batch_size, seq_length, depth]\n",
    "        Returns:\n",
    "            A Tensor with shape [batch_size, num_experts ,num_heads, seq_length, depth/(num_heads*num_experts)]\n",
    "        \"\"\"\n",
    "        if len(x.shape) != 3:\n",
    "            raise ValueError(\"x must have rank 3\")\n",
    "        shape = x.shape\n",
    "        return tf.transpose(tf.reshape(x, [shape[0], shape[1], self.num_experts, self.num_heads, \n",
    "                               shape[2]//(self.num_heads*self.num_experts)]), perm=[0, 2, 3, 1, 4])\n",
    "    \n",
    "    def _merge_heads(self, x):\n",
    "        \"\"\"\n",
    "        Merge the extra num_heads into the last dimension\n",
    "        Input:\n",
    "            x: a Tensor with shape [batch_size, num_experts ,num_heads, seq_length, depth/num_heads]\n",
    "        Returns:\n",
    "            A Tensor with shape [batch_size, seq_length, num_experts, depth/num_experts]\n",
    "        \"\"\"\n",
    "        if len(x.shape) != 5:\n",
    "            raise ValueError(\"x must have rank 5\")\n",
    "        shape = x.shape\n",
    "        return tf.reshape(tf.transpose(x, perm=[0, 3, 1, 2, 4]), [shape[0], shape[3], self.num_experts, shape[4]*self.num_heads])\n",
    "    \n",
    "    def __call__(self, queries, keys, values, mask):\n",
    "        \n",
    "        # Do a linear for each component\n",
    "        queries = self.query_linear(queries)\n",
    "        keys = self.key_linear(keys)\n",
    "        values = self.value_linear(values)\n",
    "        # Split into multiple heads\n",
    "        queries = self._split_heads(queries)\n",
    "        keys = self._split_heads(keys)\n",
    "        values = self._split_heads(values)\n",
    "        # Scale queries\n",
    "        queries *= self.query_scale\n",
    "        # Combine queries and keys\n",
    "        logits = tf.matmul(queries, keys, transpose_b=True)\n",
    "        \n",
    "        if mask is not None:\n",
    "            #logits += (mask * -1e18)\n",
    "            mask = tf.expand_dims(tf.expand_dims(mask, 1), 1)\n",
    "            logits = logits*(1 - tf.cast(mask, tf.float32))+ (-1e18) * tf.cast(mask, tf.float32)\n",
    "            \n",
    "\n",
    "        ## attention weights \n",
    "        # attetion_weights = logits.sum(dim=1)/self.num_heads\n",
    "        # Convert to probabilites\n",
    "        weights = tf.nn.softmax(logits, axis=-1)\n",
    "        # Dropout\n",
    "        weights = self.dropout(weights)\n",
    "        # Combine with values to get context\n",
    "        contexts = tf.matmul(weights, values)\n",
    "        # Merge heads\n",
    "        contexts = self._merge_heads(contexts)\n",
    "        #contexts = torch.tanh(contexts)\n",
    "        # Linear to get output\n",
    "        outputs = self.output_linear(contexts)\n",
    "        return outputs\n",
    "\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi-head attention as per https://arxiv.org/pdf/1706.03762.pdf\n",
    "    Refer Figure 2\n",
    "    \"\"\"\n",
    "    def __init__(self, input_depth, total_key_depth, total_value_depth, output_depth, \n",
    "                 num_heads, bias_mask=None, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            input_depth: Size of last dimension of input\n",
    "            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n",
    "            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n",
    "            output_depth: Size last dimension of the final output\n",
    "            num_heads: Number of attention heads\n",
    "            bias_mask: Masking tensor to prevent connections to future elements\n",
    "            dropout: Dropout probability (Should be non-zero only during training)\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Checks borrowed from \n",
    "        # https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py\n",
    "        # if total_key_depth % num_heads != 0:\n",
    "        #     raise ValueError(\"Key depth (%d) must be divisible by the number of \"\n",
    "        #                      \"attention heads (%d).\" % (total_key_depth, num_heads))\n",
    "        # if total_value_depth % num_heads != 0:\n",
    "        #     raise ValueError(\"Value depth (%d) must be divisible by the number of \"\n",
    "        #                      \"attention heads (%d).\" % (total_value_depth, num_heads))\n",
    "            \n",
    "        if total_key_depth % num_heads != 0:\n",
    "            print(\"Key depth (%d) must be divisible by the number of \"\n",
    "                             \"attention heads (%d).\" % (total_key_depth, num_heads))\n",
    "            total_key_depth = total_key_depth - (total_key_depth % num_heads)\n",
    "        if total_value_depth % num_heads != 0:\n",
    "            print(\"Value depth (%d) must be divisible by the number of \"\n",
    "                             \"attention heads (%d).\" % (total_value_depth, num_heads))\n",
    "            total_value_depth = total_value_depth - (total_value_depth % num_heads)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.query_scale = (total_key_depth//num_heads)**-0.5 ## sqrt\n",
    "        self.bias_mask = bias_mask\n",
    "        \n",
    "        # Key and query depth will be same\n",
    "        self.query_linear = layers.Dense(total_key_depth, use_bias=False)\n",
    "        self.key_linear = layers.Dense(total_key_depth, use_bias=False)\n",
    "        self.value_linear = layers.Dense(total_value_depth, use_bias=False)\n",
    "        self.output_linear = layers.Dense(output_depth, use_bias=False)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "    \n",
    "    def _split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Split x such to add an extra num_heads dimension\n",
    "        Input:\n",
    "            x: a Tensor with shape [batch_size, seq_length, depth]\n",
    "        Returns:\n",
    "            A Tensor with shape [batch_size, num_heads, seq_length, depth/num_heads]\n",
    "        \"\"\"\n",
    "        if len(x.shape) != 3:\n",
    "            raise ValueError(\"x must have rank 3\")\n",
    "        shape = x.shape\n",
    "        return tf.transpose(tf.reshape(x, [shape[0], shape[1], self.num_heads, shape[2]//self.num_heads]), perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def _merge_heads(self, x):\n",
    "        \"\"\"\n",
    "        Merge the extra num_heads into the last dimension\n",
    "        Input:\n",
    "            x: a Tensor with shape [batch_size, num_heads, seq_length, depth/num_heads]\n",
    "        Returns:\n",
    "            A Tensor with shape [batch_size, seq_length, depth]\n",
    "        \"\"\"\n",
    "        if len(x.shape) != 4:\n",
    "            raise ValueError(\"x must have rank 4\")\n",
    "        shape = x.shape\n",
    "        return tf.reshape(tf.transpose(x, perm=[0, 2, 1, 3]), [shape[0], shape[2], shape[3]*self.num_heads])\n",
    "    \n",
    "    def __call__(self, queries, keys, values, mask):\n",
    "        \n",
    "        # Do a linear for each component\n",
    "        queries = self.query_linear(queries)\n",
    "        keys = self.key_linear(keys)\n",
    "        values = self.value_linear(values)\n",
    "        # Split into multiple heads\n",
    "        queries = self._split_heads(queries)\n",
    "        keys = self._split_heads(keys)\n",
    "        values = self._split_heads(values)\n",
    "        # Scale queries\n",
    "        queries *= self.query_scale\n",
    "        # Combine queries and keys\n",
    "        logits = tf.matmul(queries, keys, transpose_b=True)\n",
    "        \n",
    "        if mask is not None:\n",
    "            #logits += (mask * -1e18)\n",
    "            mask = tf.expand_dims(mask, 1)\n",
    "            logits = logits*(1 - tf.cast(mask, tf.float32))+ (-1e18) * tf.cast(mask, tf.float32)\n",
    "\n",
    "        ## attention weights \n",
    "        attetion_weights = tf.reduce_sum(logits, axis=1)/self.num_heads\n",
    "        # Convert to probabilites\n",
    "        weights = tf.nn.softmax(logits, axis=-1)\n",
    "        # Dropout\n",
    "        weights = self.dropout(weights)\n",
    "        # Combine with values to get context\n",
    "        contexts = tf.matmul(weights, values)\n",
    "        # Merge heads\n",
    "        contexts = self._merge_heads(contexts)\n",
    "        #contexts = torch.tanh(contexts)\n",
    "        # Linear to get output\n",
    "        outputs = self.output_linear(contexts)       \n",
    "        return outputs, attetion_weights\n",
    "\n",
    "class Conv(layers.Layer):\n",
    "    \"\"\"\n",
    "    Convenience class that does padding and convolution for inputs in the format\n",
    "    [batch_size, sequence length, hidden size]\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size, kernel_size, pad_type):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            input_size: Input feature size\n",
    "            output_size: Output feature size\n",
    "            kernel_size: Kernel width\n",
    "            pad_type: left -> pad on the left side (to mask future data), \n",
    "                      both -> pad on both sides\n",
    "        \"\"\"\n",
    "        super(Conv, self).__init__()\n",
    "        padding = [kernel_size - 1, 0] if pad_type == 'left' else [kernel_size//2, (kernel_size - 1)//2]\n",
    "        self.padding = [[0, 0], [0, 0], padding]\n",
    "        self.output_size = output_size\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def __call__(self, inputs, training=True):\n",
    "        inputs = tf.pad(tf.transpose(inputs, perm=[0, 2, 1]), self.padding, \"CONSTANT\") \n",
    "        outputs = layers.Conv1D(self.output_size, self.kernel_size, padding='valid')(tf.transpose(inputs, [0, 2, 1]), training=training)\n",
    "        return outputs\n",
    "\n",
    "class PositionwiseFeedForward(layers.Layer):\n",
    "    \"\"\"\n",
    "    Does a Linear + RELU + Linear on each of the timesteps\n",
    "    \"\"\"\n",
    "    def __init__(self, input_depth, filter_size, output_depth, layer_config='ll', padding='left', dropout=0.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            input_depth: Size of last dimension of input\n",
    "            filter_size: Hidden size of the middle layer\n",
    "            output_depth: Size last dimension of the final output\n",
    "            layer_config: ll -> linear + ReLU + linear\n",
    "                          cc -> conv + ReLU + conv etc.\n",
    "            padding: left -> pad on the left side (to mask future data), \n",
    "                     both -> pad on both sides\n",
    "            dropout: Dropout probability (Should be non-zero only during training)\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        \n",
    "        self.layer_config = layer_config\n",
    "        self.linear_layer1 = layers.Dense(filter_size)\n",
    "        self.linear_layer2 = layers.Dense(output_depth)\n",
    "        self.conv_layer1 = Conv(filter_size, kernel_size=3, pad_type=padding)\n",
    "        self.conv_layer2 = Conv(output_depth, kernel_size=3, pad_type=padding)\n",
    "        self.relu_layer = layers.ReLU()\n",
    "        self.dropout_layer = layers.Dropout(dropout)\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for i, cur_layer in enumerate(self.layer_config):\n",
    "            if i < len(self.layer_config)-1:\n",
    "                if cur_layer == 'l':\n",
    "                    x = self.linear_layer1(x)\n",
    "                elif cur_layer == 'c':\n",
    "                    x = self.conv_layer1(x)\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown layer type {}\".format(cur_layer))\n",
    "            else:\n",
    "                if cur_layer == 'l':\n",
    "                    x = self.linear_layer2(x)\n",
    "                elif cur_layer == 'c':\n",
    "                    x = self.conv_layer2(x)\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown layer type {}\".format(cur_layer))\n",
    "            x = self.relu_layer(x)\n",
    "            x = self.dropout_layer(x)\n",
    "        return x\n",
    "\n",
    "def _gen_bias_mask(max_length):\n",
    "    \"\"\"\n",
    "    Generates bias values (-Inf) to mask future timesteps during attention\n",
    "    \"\"\"\n",
    "    np_mask = np.triu(np.full([max_length, max_length], -np.inf), 1)\n",
    "    tf_mask = tf.cast(np_mask, dtype=tf.float32)\n",
    "    return tf.expand_dims(tf.expand_dims(tf_mask, 0), 1)\n",
    "\n",
    "def _gen_timing_signal(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n",
    "    position = np.arange(length)\n",
    "    num_timescales = channels // 2\n",
    "    log_timescale_increment = ( math.log(float(max_timescale) / float(min_timescale)) / (float(num_timescales) - 1))\n",
    "    inv_timescales = min_timescale * np.exp(np.arange(num_timescales).astype(np.float) * -log_timescale_increment)\n",
    "    scaled_time = np.expand_dims(position, 1) * np.expand_dims(inv_timescales, 0)\n",
    "\n",
    "    signal = np.concatenate([np.sin(scaled_time), np.cos(scaled_time)], axis=1)\n",
    "    signal = np.pad(signal, [[0, 0], [0, channels % 2]], 'constant', constant_values=[0.0, 0.0])\n",
    "    signal = tf.reshape(signal, [1,length, channels])\n",
    "    #signal =  signal.reshape([1, length, channels])\n",
    "    return tf.cast(signal, dtype=tf.float32)\n",
    "\n",
    "def _get_attn_subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    Get an attention mask to avoid using the subsequent info.\n",
    "    Args:\n",
    "        size: int\n",
    "    Returns:\n",
    "        (`LongTensor`):\n",
    "        * subsequent_mask `[1 x size x size]`\n",
    "    \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    subsequent_mask = tf.cast(subsequent_mask, dtype=tf.uint8)\n",
    "    return subsequent_mask\n",
    "\n",
    "class OutputLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Abstract base class for output layer. \n",
    "    Handles projection to output labels\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(OutputLayer, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.output_projection = layers.Dense(output_size)\n",
    "\n",
    "    def loss(self, hidden, labels):\n",
    "        raise NotImplementedError('Must implement {}.loss'.format(self.__class__.__name__))\n",
    "\n",
    "class SoftmaxOutputLayer(OutputLayer):\n",
    "    \"\"\"\n",
    "    Implements a softmax based output layer\n",
    "    \"\"\"\n",
    "    def forward(self, hidden):\n",
    "        logits = self.output_projection(hidden)\n",
    "        probs = tf.nn.softmax(logits, axis=-1)\n",
    "        predictions = tf.math.argmax(probs, axis=-1)\n",
    "        return predictions\n",
    "\n",
    "    def loss(self, hidden, labels):\n",
    "        logits = self.output_projection(hidden)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        return -tf.math.reduce_sum(tf.one_hot(labels, depth=self.output_size)*tf.nn.log_softmax(log_probs, -1))/len(labels)\n",
    "\n",
    "def position_encoding(sentence_size, embedding_dim):\n",
    "    encoding = np.ones((embedding_dim, sentence_size), dtype=np.float32)\n",
    "    ls = sentence_size + 1\n",
    "    le = embedding_dim + 1\n",
    "    for i in range(1, le):\n",
    "        for j in range(1, ls):\n",
    "            encoding[i-1, j-1] = (i - (embedding_dim+1)/2) * (j - (sentence_size+1)/2)\n",
    "    encoding = 1 + 4 * encoding / embedding_dim / sentence_size\n",
    "    # Make position encoding of time words identity to avoid modifying them\n",
    "    # encoding[:, -1] = 1.0\n",
    "    return np.transpose(encoding)\n",
    "\n",
    "def gen_embeddings(vocab):\n",
    "    \"\"\"\n",
    "        Generate an initial embedding matrix for `word_dict`.\n",
    "        If an embedding file is not given or a word is not in the embedding file,\n",
    "        a randomly initialized vector will be used.\n",
    "    \"\"\"\n",
    "    embeddings = np.random.randn(vocab.n_words, 100) * 0.01 \n",
    "    print('Embeddings: %d x %d' % (vocab.n_words, 100))\n",
    "    return embeddings\n",
    "\n",
    "# class TF_Embedding(layers.Layer):\n",
    "#     def __init__(self, vocab, d_model, padding_idx=0, pretrain=True, **kwargs):\n",
    "#         super(TF_Embedding, self).__init__(**kwargs)\n",
    "#         self.input_dim = vocab.n_words\n",
    "#         self.output_dim = d_model\n",
    "#         self.padding_idx = padding_idx\n",
    "#         if pretrain:\n",
    "#             pre_embedding = gen_embeddings(vocab)\n",
    "#             self.embeddings =  layers.Embedding(self.input_dim, self.output_dim, weights=[pre_embedding])\n",
    "#         else:\n",
    "#             self.embeddings = self.add_weight(\n",
    "#                 shape=(self.input_dim, self.output_dim),\n",
    "#                 initializer='random_normal',\n",
    "#                 dtype='float32')\n",
    "\n",
    "#     def call(self, inputs): \n",
    "#         def compute_mask():\n",
    "#             return tf.not_equal(inputs, self.padding_idx)\n",
    "        \n",
    "#         out = tf.nn.embedding_lookup(self.embeddings, inputs)\n",
    "#         masking = compute_mask() # [B, T], bool\n",
    "#         masking = tf.cast(tf.tile(masking[:,:, tf.newaxis], [1,1,self.output_dim]), \n",
    "#                           dtype=tf.float32) # [B, T, D]\n",
    "#         return tf.multiply(out, masking)\n",
    "\n",
    "# class Embeddings(layers.Layer):\n",
    "#     def __init__(self, vocab, d_model, padding_idx=None, pretrain=True):\n",
    "#         super(Embeddings, self).__init__()\n",
    "#         self.lut = TF_Embedding(vocab, d_model, padding_idx=padding_idx, pretrain=pretrain)\n",
    "#         self.d_model = d_model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "# def share_embedding(vocab, pretrain=False):#修改\n",
    "#     embedding = Embeddings(vocab, 100, padding_idx=1, pretrain=pretrain)\n",
    "#     #embedding.lut.weight.data.copy_(torch.FloatTensor(pre_embedding))\n",
    "#     #embedding.lut.weight.data.requires_grad = True\n",
    "#     return embedding\n",
    "\n",
    "#修改\n",
    "class Embeddinglayer(layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        # model hyper parameter variables\n",
    "        super(Embeddinglayer, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        output = self.embedding(sequences) * tf.sqrt(tf.cast(self.d_model, dtype=tf.float32))\n",
    "        return output\n",
    "\n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return self.optimizer.state_dict()\n",
    "\n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "def get_attn_key_pad_mask(seq_k, seq_q):\n",
    "    ''' For masking out the padding part of key sequence. '''\n",
    "\n",
    "    # Expand to fit the shape of key query attention matrix.\n",
    "    len_q = seq_q.shape[1]\n",
    "    padding_mask = tf.math.equal(seq_k, 1)\n",
    "    padding_mask = tf.broadcast_to(tf.expand_dims(padding_mask, 1), [-1, len_q, -1]) # b x lq x lk\n",
    "    return padding_mask\n",
    "\n",
    "def get_input_from_batch(batch):\n",
    "    enc_batch = batch[0] #enc_batch = batch[\"input_batch\"]\n",
    "    enc_lens = batch[1] # enc_lens = batch[\"input_lengths\"]\n",
    "    batch_size, max_enc_len = enc_batch.shape\n",
    "    assert len(enc_lens) == batch_size\n",
    "\n",
    "    enc_padding_mask = tf.cast(sequence_mask(enc_lens, max_len=max_enc_len), dtype=tf.float32)\n",
    "\n",
    "    extra_zeros = None\n",
    "    enc_batch_extend_vocab = None\n",
    "\n",
    "    c_t_1 = tf.zeros((batch_size, 2 * 100))\n",
    "\n",
    "    coverage = None\n",
    "\n",
    "    return enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, c_t_1, coverage\n",
    "\n",
    "def get_output_from_batch(batch):\n",
    "    #dec_batch = batch[\"target_batch\"]\n",
    "    dec_batch = batch[3]\n",
    "    target_batch = dec_batch       \n",
    "    #dec_lens_var = batch[\"target_lengths\"]\n",
    "    dec_lens_var = batch[4]\n",
    "    #print('*******DEC BATCH*******', dec_lens_var) ----- Tensor(\"input_x_4:0\", shape=(32, 1), dtype=int64)\n",
    "    #max_dec_len = max(dec_lens_var)\n",
    "    #max_dec_len = tf.math.reduce_max(dec_lens_var)\n",
    "    max_dec_len = target_batch.shape[1]\n",
    "    dec_padding_mask = tf.cast(sequence_mask(dec_lens_var, max_len=max_dec_len), dtype=tf.float32)\n",
    "    \n",
    "    return dec_batch, dec_padding_mask, max_dec_len, dec_lens_var, target_batch\n",
    "\n",
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = tf.math.reduce_max(sequence_length)\n",
    "    batch_size = sequence_length.shape[0]\n",
    "    seq_range = tf.range(0, max_len, dtype=tf.int32)\n",
    "    seq_range_expand = tf.broadcast_to(tf.expand_dims(seq_range, 0), [batch_size, max_len])\n",
    "    seq_range_expand = seq_range_expand\n",
    "    #seq_length_expand = tf.broadcast_to(tf.expand_dims(sequence_length, 1), seq_range_expand.shape)\n",
    "    seq_length_expand = tf.broadcast_to(sequence_length, seq_range_expand.shape)\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "def print_custum(emotion,dial,ref,hyp_g,hyp_b):\n",
    "    print(\"emotion:{}\".format(emotion))\n",
    "    print(\"Context:{}\".format(dial))\n",
    "    #print(\"Topk:{}\".format(hyp_t))\n",
    "    print(\"Beam: {}\".format(hyp_b))\n",
    "    print(\"Greedy:{}\".format(hyp_g))\n",
    "    print(\"Ref:{}\".format(ref))\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(np.prod(p.get_shape()) for p in model.trainable_weights)\n",
    "\n",
    "def make_infinite(dataloader):\n",
    "    while True:\n",
    "        for x in dataloader:\n",
    "            yield x\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (..., vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "    \"\"\"\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < tf.math.top_k(logits, top_k)[0][:, -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits = tf.sort(logits, direction='DESCENDING')\n",
    "        sorted_indices = tf.argsort(logits, direction='DESCENDING')\n",
    "        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[:, 1:] = tf.identity(sorted_indices_to_remove[:, :-1])\n",
    "        sorted_indices_to_remove[:, 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "008c2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n",
    "    filter_size, max_length=1000, input_dropout=0.0, layer_dropout=0.0, attention_dropout=0.0, \n",
    "    relu_dropout=0.0, use_mask=False, universal=False):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.universal = universal\n",
    "        self.num_layers = num_layers\n",
    "        self.timing_signal = _gen_timing_signal(max_length, hidden_size)\n",
    "        \n",
    "        if(self.universal):  \n",
    "            ## for t\n",
    "            self.position_signal = _gen_timing_signal(num_layers, hidden_size)\n",
    "\n",
    "        params =(hidden_size, \n",
    "                 total_key_depth or hidden_size,\n",
    "                 total_value_depth or hidden_size,\n",
    "                 filter_size, \n",
    "                 num_heads, \n",
    "                 _gen_bias_mask(max_length) if use_mask else None,\n",
    "                 layer_dropout, \n",
    "                 attention_dropout, \n",
    "                 relu_dropout)\n",
    "        \n",
    "        self.embedding_proj = layers.Dense(hidden_size, use_bias = False)\n",
    "        if(self.universal):\n",
    "            self.enc = EncoderLayer(*params)\n",
    "        else:\n",
    "            self.enc = [EncoderLayer(*params) for _ in range(num_layers)]\n",
    "        \n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.input_dropout = layers.Dropout(input_dropout)\n",
    "\n",
    "    def __call__(self, inputs, mask, training=True): #ADHOC = TRUE\n",
    "        #Add input dropout\n",
    "        \n",
    "        inputs = tf.cast(inputs, dtype = tf.float32) #inputs dim (32, 38)\n",
    "        x = self.input_dropout(inputs)\n",
    "        \n",
    "        # Project to hidden size\n",
    "        x = self.embedding_proj(x) #x dim [32, 100]\n",
    "        \n",
    "        if(self.universal):\n",
    "            for l in range(self.num_layers):\n",
    "                x += tf.cast(self.timing_signal[:, :inputs.shape[1], :], inputs.dtype)\n",
    "                x += tf.cast(tf.tile(tf.expand_dims(self.position_signal[:, l, :], 1), [1,inputs.shape[1],1]), inputs.dtype)\n",
    "                x = self.enc(x, mask=mask, training=training)\n",
    "            y = self.layer_norm(x)\n",
    "        else:\n",
    "            x += tf.cast(self.timing_signal[:, :inputs.shape[1], :], inputs.dtype)   \n",
    "            for i in range(self.num_layers):\n",
    "                x = self.enc[i](x, mask, training=training)\n",
    "            y = self.layer_norm(x)\n",
    "        return y\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n",
    "                 filter_size, max_length=1000, input_dropout=0.0, layer_dropout=0.0, \n",
    "                 attention_dropout=0.0, relu_dropout=0.0, universal=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.universal = universal\n",
    "        self.num_layers = num_layers\n",
    "        self.timing_signal = _gen_timing_signal(max_length, hidden_size)\n",
    "        \n",
    "        if(self.universal):  \n",
    "            self.position_signal = _gen_timing_signal(num_layers, hidden_size)\n",
    "\n",
    "        self.mask = _get_attn_subsequent_mask(max_length)\n",
    "\n",
    "        params =(hidden_size, \n",
    "                 total_key_depth or hidden_size,\n",
    "                 total_value_depth or hidden_size,\n",
    "                 filter_size, \n",
    "                 num_heads, \n",
    "                 _gen_bias_mask(max_length), # mandatory\n",
    "                 layer_dropout, \n",
    "                 attention_dropout, \n",
    "                 relu_dropout)\n",
    "        \n",
    "        if(self.universal):\n",
    "            self.dec = DecoderLayer(*params)\n",
    "        else:\n",
    "            self.dec = [DecoderLayer(*params) for l in range(num_layers)]\n",
    "        \n",
    "        self.embedding_proj = layers.Dense(hidden_size, use_bias = False)\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.input_dropout = layers.Dropout(input_dropout)\n",
    "\n",
    "\n",
    "    def __call__(self, inputs, encoder_output, mask, training=True):#ADHOC = TRUE\n",
    "        mask_src, mask_trg = mask\n",
    "        dec_mask = tf.math.greater(mask_trg + self.mask[:, :mask_trg.shape[-1], :mask_trg.shape[-1]], 0)\n",
    "        #Add input dropout\n",
    "        x = self.input_dropout(inputs)\n",
    "        x = self.embedding_proj(x)\n",
    "            \n",
    "        if(self.universal):\n",
    "            x += tf.cast(self.timing_signal[:, :inputs.shape[1], :], inputs.dtype)\n",
    "            for l in range(self.num_layers):\n",
    "                x += tf.cast(tf.tile(tf.expand_dims(self.position_signal[:, l, :], 1), [1,inputs.shape[1],1]), inputs.dtype)\n",
    "                x, _, attn_dist, _ = self.dec((x, encoder_output, [], (mask_src,dec_mask)),training=training)\n",
    "            y = self.layer_norm(x)\n",
    "        else:\n",
    "            # Add timing signal\n",
    "            x += tf.cast(self.timing_signal[:, :inputs.shape[1], :], inputs.dtype) \n",
    "            # Run decoder\n",
    "            for i in range(self.num_layers):\n",
    "                x, _, attn_dist, _ = self.dec[i]((x, encoder_output, [], (mask_src,dec_mask)),training=training)\n",
    "            y = self.layer_norm(x)\n",
    "        return y, attn_dist\n",
    "\n",
    "class MulDecoder(layers.Layer):\n",
    "    def __init__(self, expert_num,  embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n",
    "                 filter_size, max_length=1000, input_dropout=0.0, layer_dropout=0.0, \n",
    "                 attention_dropout=0.0, relu_dropout=0.0):\n",
    "        super(MulDecoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.timing_signal = _gen_timing_signal(max_length, hidden_size)\n",
    "        self.mask = _get_attn_subsequent_mask(max_length)\n",
    "\n",
    "        params =(hidden_size, \n",
    "                 total_key_depth or hidden_size,\n",
    "                 total_value_depth or hidden_size,\n",
    "                 filter_size, \n",
    "                 num_heads, \n",
    "                 _gen_bias_mask(max_length), # mandatory\n",
    "                 layer_dropout, \n",
    "                 attention_dropout, \n",
    "                 relu_dropout)\n",
    "        self.experts = [DecoderLayer(*params) for e in range(expert_num)]\n",
    "        self.dec = [DecoderLayer(*params) for l in range(num_layers)]\n",
    "        \n",
    "        self.embedding_proj = layers.Dense(hidden_size, use_bias = False)\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.input_dropout = layers.Dropout(input_dropout)\n",
    "    \n",
    "    def __call__(self, inputs, encoder_output, mask, attention_epxert, training=True):#ADHOC = TRUE\n",
    "        mask_src, mask_trg = mask\n",
    "        dec_mask = tf.math.greater(tf.cast(mask_trg, tf.uint8) + self.mask[:, :mask_trg.shape[-1], :mask_trg.shape[-1]], 0)\n",
    "        #Add input dropout\n",
    "        x = self.input_dropout(inputs)\n",
    "        x = self.embedding_proj(x)\n",
    "        # Add timing signal\n",
    "        x += tf.cast(self.timing_signal[:, :inputs.shape[1], :], inputs.dtype)\n",
    "        expert_outputs = []\n",
    "\n",
    "        #compute experts\n",
    "        #TODO forward all experts in parrallel\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            expert_out , _, attn_dist, _ = expert((x, encoder_output, [], (mask_src,dec_mask)), training=training)\n",
    "            expert_outputs.append(expert_out)\n",
    "        x = tf.stack(expert_outputs, axis=1) #(batch_size, expert_number, len, hidden_size)\n",
    "        x = attention_epxert * x\n",
    "        x = tf.reduce_sum(x, axis=1)#(batch_size, len, hidden_size)\n",
    "        # Run decoder\n",
    "        for i in range(self.num_layers):\n",
    "            x, _, attn_dist, _ = self.dec[i]((x, encoder_output, [], (mask_src,dec_mask)), training=training)\n",
    "        # Final layer normalization\n",
    "        y = self.layer_norm(x)\n",
    "        return y, attn_dist\n",
    "\n",
    "class Generator(layers.Layer):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = layers.Dense(vocab)\n",
    "        self.p_gen_linear = layers.Dense(1)\n",
    "\n",
    "    def __call__(self, x, attn_dist=None, enc_batch_extend_vocab=None, extra_zeros=None, temp=1, beam_search=False, attn_dist_db=None):\n",
    "        logit = self.proj(x)\n",
    "        return tf.nn.log_softmax(logit, axis = -1)\n",
    "\n",
    "class Transformer_experts(layers.Layer):\n",
    "    def __init__(self, vocab, decoder_number, model_file_path=None, is_eval=False, load_optim=False):\n",
    "        super(Transformer_experts, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = vocab.n_words\n",
    "\n",
    "        self.embedding = Embeddinglayer(self.vocab.n_words, 100)#修改\n",
    "        self.encoder = Encoder(100, 100, num_layers=2, num_heads=2, total_key_depth=40, total_value_depth=40,\n",
    "                                filter_size=50,universal=False)\n",
    "        self.decoder_number = decoder_number\n",
    "        ## multiple decoders\n",
    "        self.decoder = MulDecoder(decoder_number, 100, 100,  num_layers=2, num_heads=2, \n",
    "                                    total_key_depth=40,total_value_depth=40,filter_size=50)\n",
    " \n",
    "        self.decoder_key = layers.Dense(decoder_number, use_bias=False)\n",
    "\n",
    "        self.generator = Generator(100, self.vocab_size)\n",
    "        self.emoji_embedding = layers.Dense(100, use_bias=False)\n",
    "        self.criterion = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction='none', label_smoothing=0.1)\n",
    "        \n",
    "        self.attention_activation =  layers.Activation('sigmoid') #nn.Softmax()\n",
    "        \n",
    "        self.optimizer = NoamOpt(100, 1, 8000, tf.keras.optimizers.Adam(lr=0, beta_1=0.9, beta_2=0.98, epsilon=1e-9))\n",
    "    \n",
    "    def __call__(self, batch, training=True): #ADHC training=True\n",
    "        enc_batch, _, _, enc_batch_extend_vocab, extra_zeros, _, _ = get_input_from_batch(batch)\n",
    "        dec_batch, _, _, _, _ = get_output_from_batch(batch)\n",
    "        ## Encode\n",
    "        mask_src = tf.expand_dims(tf.math.equal(enc_batch, 1), axis = 1)\n",
    "        emb_mask = self.embedding(batch[2]) #\"mask_input\"\n",
    "        #print('Encoder Input: ', (self.embedding(enc_batch)+emb_mask).shape)\n",
    "        encoder_outputs = self.encoder(self.embedding(enc_batch)+emb_mask, mask_src, training=training)\n",
    "        \n",
    "        ## Attention over decoder\n",
    "        q_h = encoder_outputs[:,0]\n",
    "        #q_h = encoder_outputs[:,0]\n",
    "        logit_prob = self.decoder_key(q_h) #(bsz, num_experts)\n",
    "        attention_parameters = self.attention_activation(logit_prob)\n",
    "        attention_parameters = tf.expand_dims(tf.expand_dims(attention_parameters, axis=-1), axis=-1) # (batch_size, expert_num, 1, 1)\n",
    "        # Decode \n",
    "        sos_token = tf.expand_dims(tf.cast([3] * enc_batch.shape[0], dtype=tf.int32), axis=1)\n",
    "        dec_batch_shift = tf.concat((sos_token, dec_batch[:, :-1]), axis=1)\n",
    "        mask_trg = tf.expand_dims(tf.math.equal(dec_batch_shift, 1), axis=1)\n",
    "       \n",
    "        pre_logit, attn_dist = self.decoder(self.embedding(dec_batch_shift), encoder_outputs, (mask_src,mask_trg), attention_parameters, training=training)\n",
    "        ## compute output dist\n",
    "        logit = self.generator(pre_logit, attn_dist, None, extra_zeros, attn_dist_db=None)\n",
    "        return logit, logit_prob\n",
    "    \n",
    "    def decoder_greedy(self, batch, max_dec_step=30, training=False):\n",
    "        enc_batch, _, _, enc_batch_extend_vocab, extra_zeros, _, _ = get_input_from_batch(batch)\n",
    "        mask_src = tf.expand_dims(tf.math.equal(enc_batch, 1), axis = 1)\n",
    "        emb_mask = self.embedding(batch[2]) #\"mask_input\"\n",
    "        encoder_outputs = self.encoder(self.embedding(enc_batch)+emb_mask, mask_src, training=training)\n",
    "        ## Attention over decoder\n",
    "        q_h = encoder_outputs[:,0]\n",
    "        #q_h = encoder_outputs[:,0]\n",
    "        logit_prob = self.decoder_key(q_h)    \n",
    "        attention_parameters = self.attention_activation(logit_prob)\n",
    "        attention_parameters = tf.expand_dims(tf.expand_dims(attention_parameters, axis=-1), axis=-1) # (batch_size, expert_num, 1, 1)\n",
    "\n",
    "        ys = tf.cast(tf.fill([1, 1], 3), dtype=tf.int32)#修改int64\n",
    "        mask_trg = tf.expand_dims(tf.math.equal(ys, 1), axis=1)\n",
    "        decoded_words = []\n",
    "        for i in range(max_dec_step+1):\n",
    "            out, attn_dist = self.decoder(self.embedding(ys),encoder_outputs, (mask_src,mask_trg), attention_parameters, training=training)\n",
    "            logit = self.generator(out,attn_dist,enc_batch_extend_vocab, extra_zeros, attn_dist_db=None)\n",
    "            next_word = tf.math.argmax(logit[:, -1], axis = 1)\n",
    "            decoded_words.append(['<EOS>' if ni.item() == 2 else self.vocab.index2word[ni] for ni in tf.reshape(next_word, -1).numpy()])\n",
    "            next_word = next_word[0]\n",
    "            ys = tf.concat([ys, tf.cast(tf.fill([1, 1], next_word), dtype=tf.int32)], axis=1)#修改int64\n",
    "            mask_trg = tf.expand_dims(tf.math.equal(ys, 1), axis=1)\n",
    "\n",
    "        sent = []\n",
    "        for _, row in enumerate(np.transpose(decoded_words)):\n",
    "            st = ''\n",
    "            for e in row:\n",
    "                if e == '<EOS>': \n",
    "                    break\n",
    "                else: \n",
    "                    st+= e + ' '\n",
    "            sent.append(st)\n",
    "        return sent\n",
    "    \n",
    "    def decoder_topk(self, batch, max_dec_step=30, training=False):\n",
    "        enc_batch, _, _, enc_batch_extend_vocab, extra_zeros, _, _ = get_input_from_batch(batch)\n",
    "        mask_src = tf.expand_dims(tf.math.equal(enc_batch, 1), axis = 1)\n",
    "        emb_mask = self.embedding(batch[2]) #\"mask_input\"\n",
    "        encoder_outputs = self.encoder(self.embedding(enc_batch)+emb_mask, mask_src, training=training)\n",
    "        ## Attention over decoder\n",
    "        q_h = encoder_outputs[:,0]\n",
    "        #q_h = encoder_outputs[:,0]\n",
    "        logit_prob = self.decoder_key(q_h)\n",
    "\n",
    "        attention_parameters = self.attention_activation(logit_prob)\n",
    "        attention_parameters = tf.expand_dims(tf.expand_dims(attention_parameters, axis=-1), axis=-1) # (batch_size, expert_num, 1, 1)\n",
    "\n",
    "        ys = tf.cast(tf.fill([1, 1], 3), dtype=tf.int32)#修改int64\n",
    "        mask_trg = tf.expand_dims(tf.math.equal(ys, 1), axis=1)\n",
    "        decoded_words = []\n",
    "        for i in range(max_dec_step+1):\n",
    "            out, attn_dist = self.decoder(self.embedding(ys),encoder_outputs, (mask_src,mask_trg), attention_parameters, training=training)\n",
    "            logit = self.generator(out,attn_dist,enc_batch_extend_vocab, extra_zeros, attn_dist_db=None)\n",
    "            filtered_logit = top_k_top_p_filtering(logit[:, -1], top_k=3, top_p=0, filter_value=-float('Inf'))\n",
    "            # Sample from the filtered distribution\n",
    "            next_word = tf.squeeze(tf.random.categorical(tf.nn.softmax(filtered_logit, axis=-1), 1))\n",
    "            decoded_words.append(['<EOS>' if ni.item() == 2 else self.vocab.index2word[ni] for ni in tf.reshape(next_word, -1).numpy()])\n",
    "            next_word = next_word[0]\n",
    "\n",
    "            ys = tf.concat([ys, tf.cast(tf.fill([1, 1], next_word), dtype=tf.int32)], axis=1)#修改int64\n",
    "            mask_trg = tf.expand_dims(tf.math.equal(ys, 1), axis=1)\n",
    "\n",
    "        sent = []\n",
    "        for _, row in enumerate(np.transpose(decoded_words)):\n",
    "            st = ''\n",
    "            for e in row:\n",
    "                if e == '<EOS>':\n",
    "                    break\n",
    "                else:\n",
    "                    st+= e + ' '\n",
    "            sent.append(st)\n",
    "        return sent\n",
    "\n",
    "class ACT_basic(layers.Layer):\n",
    "    def __init__(self,hidden_size):\n",
    "        super(ACT_basic, self).__init__()\n",
    "        self.sigma = layers.Activation('sigmoid')\n",
    "        self.p = layers.Dense(1, bias_initializer='ones')\n",
    "        self.threshold = 1 - 0.1\n",
    "    \n",
    "    def __call__(self, state, inputs, fn, time_enc, pos_enc, max_hop, encoder_output=None, decoding=False):\n",
    "        # init_hdd\n",
    "        ## [B, S]\n",
    "        halting_probability = tf.zeros(inputs.shape[0],inputs.shape[1])\n",
    "        ## [B, S\n",
    "        remainders = tf.zeros(inputs.shape[0],inputs.shape[1])\n",
    "        ## [B, S]\n",
    "        n_updates = tf.zeros(inputs.shape[0],inputs.shape[1])\n",
    "        ## [B, S, HDD]\n",
    "        previous_state = tf.zeros(inputs.shape)\n",
    "        step = 0\n",
    "        # for l in range(self.num_layers):\n",
    "        while( ((halting_probability<self.threshold) & (n_updates < max_hop)).byte().any()):\n",
    "            # Add Timing Signal\n",
    "            state = state + tf.cast(time_enc[:, :inputs.shape[1], :], inputs.dtype)\n",
    "            state = state + tf.cast(tf.tile(tf.expand_dims(pos_enc[:, step, :], 1), [1,inputs.shape[1],1]), inputs.dtype)\n",
    "\n",
    "            p = tf.squeeze(self.sigma(self.p(state)), axis=-1)\n",
    "            # Mask for inputs which have not halted yet\n",
    "            still_running = tf.cast((halting_probability < 1.0), dtype=tf.float32)\n",
    "            # Mask of inputs which halted at this step\n",
    "            new_halted = tf.cast((halting_probability + p * still_running > self.threshold), dtype=tf.float32) * still_running\n",
    "            # Mask of inputs which haven't halted, and didn't halt this step\n",
    "            still_running = tf.cast((halting_probability + p * still_running <= self.threshold), dtype=tf.float32) * still_running\n",
    "            # Add the halting probability for this step to the halting\n",
    "            # probabilities for those input which haven't halted yet\n",
    "            halting_probability = halting_probability + p * still_running\n",
    "            # Compute remainders for the inputs which halted at this step\n",
    "            remainders = remainders + new_halted * (1 - halting_probability)\n",
    "            # Add the remainders to those inputs which halted at this step\n",
    "            halting_probability = halting_probability + new_halted * remainders\n",
    "            # Increment n_updates for all inputs which are still running\n",
    "            n_updates = n_updates + still_running + new_halted\n",
    "            # Compute the weight to be applied to the new state and output\n",
    "            # 0 when the input has already halted\n",
    "            # p when the input hasn't halted yet\n",
    "            # the remainders when it halted this step\n",
    "            update_weights = p * still_running + new_halted * remainders\n",
    "            if(decoding):\n",
    "                state, _, attention_weight = fn((state,encoder_output,[]))\n",
    "            else:\n",
    "                # apply transformation on the state\n",
    "                state = fn(state)\n",
    "            # update running part in the weighted state and keep the rest\n",
    "            previous_state = ((state * tf.expand_dims(update_weights, axis=-1)) + (previous_state * (1 - tf.expand_dims(update_weights, axis=-1))))\n",
    "            if(decoding):\n",
    "                if(step==0): \n",
    "                    previous_att_weight = tf.zeros(attention_weight.shape)## [B, S, src_size]\n",
    "            previous_att_weight = ((attention_weight * tf.expand_dims(update_weights, axis=-1)) + (previous_att_weight * (1 - tf.expand_dims(update_weights, axis=-1))))\n",
    "            ## previous_state is actually the new_state at end of hte loop \n",
    "            ## to save a line I assigned to previous_state so in the next \n",
    "            ## iteration is correct. Notice that indeed we return previous_state\n",
    "            step+=1\n",
    "        if(decoding):\n",
    "            return previous_state, previous_att_weight, (remainders,n_updates)\n",
    "        else:\n",
    "            return previous_state, (remainders,n_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "22a1b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer_experts(vocab,decoder_number=program_number)\n",
    "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)#修改\n",
    "#optimizer = NoamOpt(100, 1, 8000, tf.keras.optimizers.Adam(lr=0, beta_1=0.9, beta_2=0.98, epsilon=1e-9))\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "training_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "training_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')#修改\n",
    "testing_loss = tf.keras.metrics.Mean('validation_loss', dtype=tf.float32)\n",
    "testing_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('validation_accuracy')#修改\n",
    "\n",
    "#@tf.function \n",
    "def train_step(input_x, training = True):\n",
    "    dec_batch, _, _, _, _ = get_output_from_batch(input_x)\n",
    "    if training:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logit, logit_prob = model(input_x, training)\n",
    "            train_loss = criterion(tf.reshape(dec_batch, -1), tf.reshape(logit, [-1, logit.shape[-1]])) + criterion(tf.cast(input_x[6], dtype=tf.int32), logit_prob)\n",
    "        gradients = tape.gradient(train_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        #train_pred_program = np.argmax(logit_prob.numpy(), axis=1)\n",
    "        training_loss.update_state(train_loss)\n",
    "        training_accuracy.update_state(input_x[6], logit_prob)\n",
    "    else:\n",
    "        logit, logit_prob = model(input_x, training)\n",
    "        test_loss = criterion(tf.reshape(dec_batch, -1), tf.reshape(logit, [-1, logit.shape[-1]])) + criterion(tf.cast(input_x[6], dtype=tf.int32), logit_prob)\n",
    "        #test_pred_program = np.argmax(logit_prob.numpy(), axis=1)\n",
    "        testing_loss.update_state(test_loss)\n",
    "        testing_accuracy.update_state(input_x[6], logit_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "f0eb19cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1\n",
      "step 2\n",
      "step 3\n",
      "step 4\n",
      "step 5\n",
      "step 6\n",
      "step 7\n",
      "step 8\n",
      "step 9\n",
      "step 10\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    train_step(next(data_iter), training = True)\n",
    "    print('step {}'.format(str(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "c26fff51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 curr loss:  14.215617\n",
      "step 2 curr loss:  14.658287\n",
      "step 3 curr loss:  14.073604\n",
      "step 4 curr loss:  14.045111\n",
      "step 5 curr loss:  14.289752\n",
      "step 6 curr loss:  13.947664\n",
      "step 7 curr loss:  13.951778\n",
      "step 8 curr loss:  13.64507\n",
      "step 9 curr loss:  13.47399\n",
      "step 10 curr loss:  13.636381\n",
      "step 11 curr loss:  13.534122\n",
      "step 12 curr loss:  13.414142\n",
      "step 13 curr loss:  13.393187\n",
      "step 14 curr loss:  13.125283\n",
      "step 15 curr loss:  13.144095\n",
      "step 16 curr loss:  13.342746\n",
      "step 17 curr loss:  13.382704\n",
      "step 18 curr loss:  13.081802\n",
      "step 19 curr loss:  12.83449\n",
      "step 20 curr loss:  13.311848\n"
     ]
    }
   ],
   "source": [
    "training=True\n",
    "model = Transformer_experts(vocab,decoder_number=program_number)\n",
    "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)#修改\n",
    "#optimizer = NoamOpt(100, 1, 8000, tf.keras.optimizers.Adam(lr=0, beta_1=0.9, beta_2=0.98, epsilon=1e-9))\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "training_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "training_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')#修改\n",
    "testing_loss = tf.keras.metrics.Mean('validation_loss', dtype=tf.float32)\n",
    "testing_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('validation_accuracy')#修改\n",
    "\n",
    "for i in range(20):\n",
    "    cur_input=next(data_iter)\n",
    "    dec_batch, _, _, _, _ = get_output_from_batch(cur_input)\n",
    "    if training:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logit, logit_prob = model(cur_input, training)\n",
    "            train_loss = criterion(tf.reshape(dec_batch, -1), tf.reshape(logit, [-1, logit.shape[-1]])) + criterion(tf.cast(input_x[6], dtype=tf.int32), logit_prob)\n",
    "        gradients = tape.gradient(train_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        #train_pred_program = np.argmax(logit_prob.numpy(), axis=1)\n",
    "        training_loss.update_state(train_loss)\n",
    "        training_accuracy.update_state(cur_input[6], logit_prob)\n",
    "        print('step {}'.format(str(i+1)), 'curr loss: ', train_loss.numpy())\n",
    "    else:\n",
    "        logit, logit_prob = model(cur_input, training)\n",
    "        test_loss = criterion(tf.reshape(dec_batch, -1), tf.reshape(logit, [-1, logit.shape[-1]])) + criterion(tf.cast(input_x[6], dtype=tf.int32), logit_prob)\n",
    "        #test_pred_program = np.argmax(logit_prob.numpy(), axis=1)       \n",
    "        testing_loss.update_state(test_loss)\n",
    "        testing_accuracy.update_state(cur_input[6], logit_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
